{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a logistic regression function that can be used other places too\n",
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    # identifying the shape of the input\n",
    "    num_features = int(X.get_shape()[1])\n",
    "    num_classes = int(y.get_shape()[1])\n",
    "    #n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            # initializing the weights, default\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_normal([num_features,num_classes ], stddev=0.01, seed=seed)\n",
    "            \n",
    "            # defining the weights\n",
    "            weights = tf.Variable(initializer, name=\"weights\")\n",
    "            # initializing zero vectors\n",
    "            bias = tf.Variable(tf.zeros([1, num_classes]), name=\"bias\")\n",
    "            # fundamental function to be added to softmax\n",
    "            logits = tf.matmul(X, weights, name=\"logits\") + bias \n",
    "            # softmax function\n",
    "            y_predicted = tf.nn.softmax(logits, name=\"softmax\")\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            # softmax regression with cross entropy loss\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='cross_entropy_loss')\n",
    "            # final loss\n",
    "            loss = tf.reduce_mean(entropy)\n",
    "            # using adam optimizer to minimize loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "            \n",
    "            # loss summary for tensorboard\n",
    "            loss_summary = tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_predicted, loss, optimizer, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a function for log directories of tensorboard\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"./{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = log_dir(\"logregression\")\n",
    "\n",
    "\n",
    "# Creating placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, shape=([None, 784]), name='X_placeholder') \n",
    "Y = tf.placeholder(tf.int32, shape=([None, 10]), name='Y_placeholder')\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "n_epochs = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the computation graph\n",
    "\n",
    "y_pred, loss, optimizer, loss_summary, init, saver= logistic_regression(X, Y)\n",
    "\n",
    "\n",
    "# Writing it to tensorboard\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
