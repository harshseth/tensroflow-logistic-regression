{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a logistic regression function that can be used other places too\n",
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    # identifying the shape of the input\n",
    "    num_features = int(X.get_shape()[1])\n",
    "    num_classes = int(y.get_shape()[1])\n",
    "    #n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            # initializing the weights, default\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_normal([num_features,num_classes ], stddev=0.01, seed=seed)\n",
    "            \n",
    "            # defining the weights\n",
    "            weights = tf.Variable(initializer, name=\"weights\")\n",
    "            # initializing zero vectors\n",
    "            bias = tf.Variable(tf.zeros([1, num_classes]), name=\"bias\")\n",
    "            # fundamental function to be added to softmax\n",
    "            logits = tf.matmul(X, weights, name=\"logits\") + bias \n",
    "            # softmax function\n",
    "            y_predicted = tf.nn.softmax(logits, name=\"softmax\")\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            # softmax regression with cross entropy loss\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='cross_entropy_loss')\n",
    "            # final loss\n",
    "            loss = tf.reduce_mean(entropy)\n",
    "            # using adam optimizer to minimize loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "            \n",
    "            # loss summary for tensorboard\n",
    "            loss_summary = tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_predicted, loss, optimizer, loss_summary, init, saver, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a function for log directories of tensorboard\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"./{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = log_dir(\"logregression\")\n",
    "\n",
    "\n",
    "# Creating placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, shape=([None, 784]), name='X_placeholder') \n",
    "Y = tf.placeholder(tf.int32, shape=([None, 10]), name='Y_placeholder')\n",
    "\n",
    "# Define paramaters for the model\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "n_epochs = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the computation graph\n",
    "y_pred, loss, optimizer, loss_summary, init, saver, weights= logistic_regression(X, Y, learning_rate=lr)\n",
    "\n",
    "\n",
    "# Writing it to tensorboard\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Summary for epoch loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.688596247307\n",
      "Average loss epoch 1: 0.369799320098\n",
      "(784, 10)\n",
      "[[-0.00280775 -0.00137752 -0.0067633  ...,  0.01381016 -0.01455798\n",
      "  -0.00246214]\n",
      " [-0.01360841  0.0108796  -0.00351166 ..., -0.00752883 -0.00433573\n",
      "   0.02453853]\n",
      " [ 0.00313744 -0.00527362  0.00852499 ...,  0.00886036 -0.00659783\n",
      "   0.02701756]\n",
      " ..., \n",
      " [ 0.00467879 -0.00443958 -0.01910986 ..., -0.00420249 -0.00103485\n",
      "  -0.00468861]\n",
      " [-0.01132285 -0.00246789  0.00394058 ...,  0.0023485  -0.01456116\n",
      "   0.00449873]\n",
      " [ 0.00181303  0.01575438  0.00934678 ...,  0.02459529  0.01033248\n",
      "  -0.03450175]]\n",
      "Total time: 1.25560688972 seconds\n",
      "Saving File\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Code for training module with mini batch gradient descent\n",
    "varinit = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    # initializing variables\n",
    "    varinit.run()\n",
    "    # extracting the number of batches \n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # train the model n_epochs times\n",
    "    for i in range(n_epochs):\n",
    "        total_loss =0\n",
    "        \n",
    "        # for all the batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            optimizer_val, batch_loss_val, batch_loss_summary_val = sess.run([optimizer, loss, loss_summary], feed_dict={X: X_batch, Y:Y_batch})\n",
    "            step_num = i * n_batches + batch_index\n",
    "            total_loss += batch_loss_val\n",
    "             # write loss to the plot\n",
    "            file_writer.add_summary(batch_loss_summary_val, step_num)\n",
    "        \n",
    "        # Average loss per ecpoch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    \n",
    "    best_weights = weights.eval()\n",
    "    print best_weights.shape\n",
    "    print best_weights\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print ('Saving File')\n",
    "    save_path = saver.save(sess, \"./models/log_regression.ckpt\")\n",
    "    print('Optimization Finished!') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/log_regression.ckpt\n",
      "[7 2 1 ..., 4 5 6]\n",
      "Precision Score\n",
      "0.9131\n",
      "Recall Score\n",
      "0.9131\n",
      "Accuracy Score\n",
      "0.9131\n",
      "Confusion Matrix\n",
      "[[ 962    0    1    1    0    2    8    1    5    0]\n",
      " [   0 1112    2    4    1    1    4    0   11    0]\n",
      " [  12   10  902   18   13    1   14   13   44    5]\n",
      " [   4    2   21  905    0   26    4   12   26   10]\n",
      " [   2    1    4    1  915    0   14    1    9   35]\n",
      " [  11    6    4   34   13  747   18    6   44    9]\n",
      " [  15    3    4    0   10   12  909    1    4    0]\n",
      " [   2   18   24    4   12    0    0  926    4   38]\n",
      " [  14    8    9   19    9   19   12   13  859   12]\n",
      " [  15    7    4   13   43    8    0   18    7  894]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "# Importing sklearns metrics\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # restore the training session\n",
    "    saver.restore(sess, \"./models/log_regression.ckpt\")\n",
    "    # make predictions on test set\n",
    "    '''\n",
    "    The softmax classifier predicts value for each class and sum of all the probs is 1\n",
    "    y_pred[i] contains probability score for each class, for the test instance X[i]\n",
    "    e.g y_pred[0] = [  1.85683282e-04   9.23909568e-07   2.67304393e-04   2.62420531e-03\n",
    "    4.23794518e-05   1.35844122e-04   3.43982242e-06   9.93277133e-01\n",
    "    2.15818320e-04   3.24724824e-03]\n",
    "    \n",
    "    To extract the index of the highest scoring class, we use argmax across axis 1\n",
    "    argmax(y_pred_val, 1) --> 7\n",
    "    ''' \n",
    "    # \n",
    "    # \n",
    "    y_pred_argmax = tf.argmax(y_pred,1)\n",
    "    y_pred_val = y_pred_argmax.eval(feed_dict={X:mnist.test.images, Y:mnist.test.labels})\n",
    "    \n",
    "    '''\n",
    "    For true labels, the input is a one hot encoded vector, Y[0]= [0 0 0 0 0 0 1 0 0]\n",
    "    If we take an argmax of the above Y[0] across axis 1\n",
    "        index_location = tf.argmax(Y[0],1)\n",
    "        print (index_location) --> 7\n",
    "    \n",
    "    '''\n",
    "    y_true = tf.argmax(Y,1)\n",
    "    # evaluate true value\n",
    "    y_true_val = y_true.eval(feed_dict={X:mnist.test.images, Y:mnist.test.labels})\n",
    "    print y_true_val\n",
    "    print(\"Precision Score\")\n",
    "    print precision_score(y_true_val,y_pred_val, average=\"micro\")\n",
    "    print(\"Recall Score\")\n",
    "    print recall_score(y_true_val,y_pred_val, average=\"micro\")\n",
    "    print(\"Accuracy Score\")    \n",
    "    print accuracy_score(y_true_val,y_pred_val)\n",
    "    print(\"Confusion Matrix\")    \n",
    "    print confusion_matrix(y_true_val,y_pred_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
